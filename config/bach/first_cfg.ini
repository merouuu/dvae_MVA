[User]
logger_type = 1
print_model = True
# Nouveau dossier de sauvegarde pour ne pas écraser le modèle ECG
saved_root = /content/drive/MyDrive/DVAE/saved_model_bach
# Le fichier .npz généré par votre script build_dataset.py
data_path = /content/drive/MyDrive/DVAE/bach_data.npz

[Network]
name = VRNN
# IMPORTANT : 88 dimensions pour le piano (au lieu de 1)
x_dim = 88
# Un peu plus d'espace latent pour capturer l'harmonie
z_dim = 32
activation = tanh

# Adaptation des couches denses :
# On passe de 88 (input) vers 128 puis 64.
# Si on gardait "32,32", on perdrait trop d'infos dès la première couche.
dense_x = 128,64
dense_z = 32,32

dense_hx_z =
dense_hz_x =
dense_h_z =

# RNN un peu plus costaud pour gérer la polyphonie (4 voix en même temps)
dim_RNN = 128
num_RNN = 1

# On garde du dropout pour forcer l'usage de Z, mais 0.3 est souvent suffisant pour la musique
dropout_p = 0.3

tag = VRNN_BACH

[Training]
use_cuda = True
optimization = adam

# Beta = 1 est standard pour la musique (on veut de la diversité).
# Beta = 2 risque de trop lisser la musique (la rendre ennuyeuse).
beta = 1

lr = 0.0005
epochs = 100
early_stop_patience = 15
save_frequency = 5
ss_step = 20

[DataFrame]
dataset_name = Bach
shuffle = True
seed = 0
batch_size = 64
num_workers = 2

# 300 est trop long pour apprendre la structure musicale au début.
# 64 frames (à 16Hz) = 4 secondes, c'est idéal pour apprendre des phrases musicales.
sequence_len = 64

# OBLIGATOIRE : True pour que le Dataset coupe des bouts aléatoires dans les morceaux
use_random_seq = True